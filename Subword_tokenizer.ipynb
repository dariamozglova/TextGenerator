{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:49:07.245916: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 11:49:07.245957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 11:49:07.246631: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 11:49:07.251882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 11:49:07.822481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ipunb/for keras_fant/data_fant.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110763, 1)\n",
      "(106793, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.drop_duplicates(subset='description', inplace=True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>множество творений дивных и странных оставалос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>агдалон мы все ждем тебя сейчас капитан сейча...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>смерть глядит ему в самые очи и ухмыляется гн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>брызги уха разлетаются во все стороны гоблин ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>на кончике вяло загорается пламя затем хлопае...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  множество творений дивных и странных оставалос...\n",
       "1   агдалон мы все ждем тебя сейчас капитан сейча...\n",
       "2   смерть глядит ему в самые очи и ухмыляется гн...\n",
       "3   брызги уха разлетаются во все стороны гоблин ...\n",
       "4   на кончике вяло загорается пламя затем хлопае..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:49:52.899598: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.934675: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.934878: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.935791: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.935927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.936048: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.998459: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.998627: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.998772: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-05 11:49:52.998880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 248 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "keras_dataset = tf.data.Dataset.from_tensor_slices((dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 17s, sys: 5.54 s, total: 9min 22s\n",
      "Wall time: 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ru_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    keras_dataset.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '[', ']', 'm', 'n', 'u', 'а']\n",
      "['может', '##т', 'теперь', 'тебя', '##н', 'под', '##ю', 'были', 'вот', 'там']\n",
      "['мастер', 'прочь', 'воздухе', 'берегу', 'боль', 'другом', 'любои', 'настолько', '##ении', 'четыре']\n",
      "['родом', 'сели', 'строить', 'требовалось', '##[', '##]', '##m', '##n', '##u', '##ъ']\n"
     ]
    }
   ],
   "source": [
    "print(ru_vocab[:10])\n",
    "print(ru_vocab[100:110])\n",
    "print(ru_vocab[1000:1010])\n",
    "print(ru_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('ru_vocab.txt', ru_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_vocab = open('ru_vocab.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_tokenizer = text.BertTokenizer('ru_vocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "множество творений дивных и странных оставалось еще в мире но немало было и лихого жуткого орки тролли драконы и хищные твари в лесах бродили неведомые и мудрые создания гномы трудились в горах терпеливым искусством творя из металла и камня вещи которым не было равных но близилось владычество людей и все менялось вороток на баллисте никак не хотел отпустить тетиву беатор в отчаянии ковырял его ножом впрочем безо всякого успеха эльф опустошил половину колчана и ни разу не промахнулся но его стрелы одна за другой отскакивали от гоблинских доспехов кое что кажется застряло во вражьей шкуре но что им исчадью ангмара эльфийские стрелы не страшнее шильных тычков заговоренные что ли с суеверным ужасом бормочет кэбидж свинцовые шарики из его пращи отлетают от громадин гоблинов не причиняя им ни малейшего вреда жуть какая гоблины переростки они же не бывают такими здоровыми просто не бывают никогда никогда \n",
      " агдалон мы все ждем тебя сейчас капитан сейчас сейчас в сырую погоду магический посох не работает один из гоблинов на равных рубился с дунаданом торном оба рослые медлительные и могучие обрабатывали друг друга точь в точь два добрых кузнеца лупящих по наковальне те тень бам те тень бам те тень бам кто кого возьмет измором данно акайн черноволосый красавчик получил удар гоблинским кулаком в грудь и был бы кулак голым полбеды а то ведь на нем перчатка с медными нашивками лежит арнорец в кустах стонет подняться пробует и толку от него ровным счетом никакого толк если и есть то исключительно от двух бойцов сам капитан хаддар молча отбивается от второго гоблина раза в два выше его ростом и раза в полтора шире щит его расколот левая рука висит плетью из предплечья хлещет кровь дела капитана совсем нехороши \n",
      " смерть глядит ему в самые очи и ухмыляется гном табарин отчаянно сквернословя рассерженным петушком наскакивает на третьего гоблина агдалон сейчас во от громкий шип и от верхушки его посоха отрывается легкое облачко пара по обличию своему пар этот кровная родня банному третий гоблин ловким ударом сбивает с табарина шлем но тот ничуть не замедляет бешеных наскоков наконец беатор призвав илуватара и все его небесное воинство на помощь бьет деревянным молотком по непослушному воротку тот разлетается на части содрогается станина баллисты и с верхнего самострела сходит тяжелое метательное копье зато на нижнем заедает пусковой крюк копье летит кажется прямо в лоб гоблину сражающемуся с хаддаром но потом оно попадает почему то совсем не в лоб а в огромное гоблинское ухо \n"
     ]
    }
   ],
   "source": [
    "for examples in keras_dataset.batch(3).take(1):\n",
    "  for ex in examples.numpy():\n",
    "    for i in ex:\n",
    "      print(i.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1503, 26, 773, 2272, 332, 13, 600, 167, 17, 6850, 1192, 70, 11, 746, 45, 1780, 64, 17, 116, 6978, 114, 15, 656, 2006, 726, 3280, 4325, 17, 29, 5478, 150, 2467, 11, 4924, 10, 4543, 393, 40, 1165, 1753, 277, 17, 20, 3950, 4021, 4964, 561, 26, 4921, 504, 11, 2199, 3340, 3441, 2723, 6467, 58, 26, 773, 2362, 51, 5072, 17, 1083, 1346, 948, 40, 64, 24, 2492, 167, 45, 10, 139, 5264, 11, 7132, 77, 1515, 280, 17, 52, 86, 1394, 1062, 197, 41, 6754, 3819, 3985, 53, 957, 40, 347, 60, 1907, 348, 6197, 62, 10, 53, 1745, 554, 11, 3878, 702, 839, 2362, 124, 48, 4712, 620, 170, 75, 3443, 27, 6523, 44, 672, 22, 1556, 5463, 2175, 4535, 702, 124, 625, 149, 17, 85, 2427, 40, 569, 4310, 45, 48, 1214, 416, 57, 357, 60, 1639, 6079, 60, 12, 5579, 1356, 455, 87, 6523, 93, 482, 42, 498, 57, 7103, 2936, 130, 11, 270, 6948, 46, 32, 179, 627, 45, 42, 138, 17, 199, 625, 1961, 106, 5133, 330, 4986, 3035, 1214, 40, 25, 3492, 3565, 32, 2638, 167, 54, 6327, 57, 3643, 1269, 42, 116, 25, 25, 62, 686, 724, 161, 4929, 5335, 4146, 75, 1493, 18, 615, 2527, 251, 1278, 25, 4531, 2000, 277, 4656, 1559, 51, 48, 23, 270, 2128, 60, 7025, 372, 60, 6058, 2733, 339, 12, 5579, 1356, 93, 40, 7498, 1473, 138, 85, 6276, 6112, 15, 1988, 617, 12, 5579, 3322, 23, 724, 3766, 1148, 115, 55, 65, 40, 6220, 2043, 16, 2724, 5462, 169, 40, 6220, 219, 219]\n",
      "[9, 330, 911, 541, 73, 52, 15, 2325, 103, 142, 1719, 142, 142, 11, 25, 77, 3971, 56, 114, 588, 6245, 2548, 2446, 40, 2658, 173, 140, 51, 12, 5579, 1356, 93, 41, 24, 2492, 167, 24, 6752, 386, 25, 13, 2256, 6646, 371, 3125, 371, 935, 24, 3273, 1677, 20, 3887, 3439, 6077, 17, 321, 1520, 214, 270, 858, 7197, 46, 316, 527, 49, 766, 11, 49, 766, 319, 6984, 4326, 44, 19, 62, 557, 2091, 56, 41, 6097, 146, 267, 348, 775, 6754, 58, 348, 775, 6754, 58, 348, 775, 6754, 58, 95, 344, 130, 5266, 3440, 51, 1494, 1242, 91, 1115, 9, 6372, 104, 31, 3598, 5459, 18, 270, 419, 177, 1792, 1656, 929, 12, 5579, 1356, 1106, 4662, 11, 1538, 17, 74, 66, 4772, 12, 1276, 301, 1061, 1541, 857, 9, 49, 185, 41, 366, 23, 724, 2665, 132, 25, 20, 3887, 326, 769, 4945, 83, 2182, 5304, 4791, 556, 11, 18, 6927, 290, 2361, 575, 2745, 569, 1234, 173, 17, 3011, 60, 98, 24, 5749, 4008, 67, 1104, 7469, 159, 71, 17, 143, 49, 5754, 60, 610, 6575, 193, 1719, 2754, 251, 4982, 684, 60, 5283, 1325, 60, 3787, 12, 5579, 3377, 2980, 11, 319, 958, 48, 3642, 17, 2980, 11, 1061, 4802, 3367, 3379, 48, 24, 2913, 492, 2755, 19, 4307, 802, 11, 2155, 237, 23, 1332, 827, 51, 23, 1919, 557, 598, 1965, 29, 598, 681, 101, 623, 693, 7212, 243, 40, 6978, 1472, 1041]\n",
      "[797, 7567, 101, 84, 11, 1424, 22, 674, 17, 27, 80, 1440, 4282, 535, 597, 5474, 339, 2332, 25, 159, 5825, 199, 2302, 92, 24, 2913, 1195, 155, 2786, 23, 173, 62, 4575, 127, 132, 115, 959, 41, 7057, 12, 5579, 3377, 9, 330, 911, 541, 142, 130, 60, 6058, 722, 32, 6834, 17, 60, 11, 7023, 2441, 48, 2446, 44, 60, 6052, 547, 777, 53, 214, 135, 2810, 1962, 56, 214, 139, 674, 106, 1048, 874, 145, 18, 1051, 191, 4402, 655, 6754, 5026, 1790, 12, 5579, 1356, 19, 7289, 58, 3809, 25, 5283, 520, 25, 597, 5474, 634, 3390, 45, 181, 3371, 40, 57, 1498, 251, 2094, 10, 53, 4671, 80, 127, 492, 418, 228, 10, 53, 1745, 554, 229, 515, 3050, 6846, 17, 52, 48, 1906, 5899, 1288, 420, 41, 907, 10, 862, 101, 2854, 5532, 3603, 161, 3258, 1443, 281, 56, 40, 3733, 199, 918, 1990, 294, 1062, 179, 181, 121, 7025, 547, 41, 1741, 129, 251, 1472, 6900, 25, 2591, 634, 6754, 3819, 3985, 77, 17, 25, 11, 7023, 1492, 2300, 7103, 808, 25, 2559, 7279, 20, 6501, 1769, 53, 2069, 1663, 41, 85, 1278, 953, 57, 3120, 173, 23, 4284, 418, 148, 18, 3587, 159, 2069, 383, 237, 498, 345, 11, 2405, 12, 5579, 5783, 25, 270, 1334, 2965, 294, 63, 25, 2754, 251, 4982, 67, 45, 119, 287, 56, 6128, 184, 49, 243, 40, 11, 2405, 9, 11, 4073, 12, 5579, 1356, 1371, 3104]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = ru_tokenizer.tokenize(examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(1,-1) #(-2,-1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_string(ints):\n",
    "  strs = [chr(i) for i in ints]\n",
    "  joined = [''.join(strs)]\n",
    "  return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['множество\\n т\\n ##во\\n ##рен\\n ##ии\\n д\\n ##ив\\n ##ных\\n и\\n странных\\n оставалось\\n еще\\n в\\n мире\\n но\\n немало\\n было\\n и\\n ли\\n ##хо\\n ##го\\n ж\\n ##ут\\n ##кого\\n орки\\n тролли\\n драконы\\n и\\n х\\n ##ищ\\n ##ные\\n твари\\n в\\n лесах\\n б\\n ##род\\n ##или\\n не\\n ##ве\\n ##дом\\n ##ые\\n и\\n м\\n ##уд\\n ##рые\\n создания\\n гномы\\n т\\n ##руд\\n ##ились\\n в\\n горах\\n тер\\n ##пели\\n ##вым\\n искусство\\n ##м\\n т\\n ##во\\n ##ря\\n из\\n металла\\n и\\n камня\\n вещи\\n которым\\n не\\n было\\n р\\n ##ав\\n ##ных\\n но\\n б\\n ##ли\\n ##зилось\\n в\\n ##лад\\n ##ы\\n ##чество\\n людеи\\n и\\n все\\n меня\\n ##лось\\n ворот\\n ##ок\\n на\\n ба\\n ##лл\\n ##ист\\n ##е\\n никак\\n не\\n хотел\\n от\\n ##пустить\\n те\\n ##тив\\n ##у\\n б\\n ##е\\n ##ат\\n ##ор\\n в\\n отчаянии\\n ко\\n ##вы\\n ##ря\\n ##л\\n его\\n ножом\\n впрочем\\n без\\n ##о\\n всякого\\n у\\n ##спех\\n ##а\\n эльф\\n о\\n ##пу\\n ##сто\\n ##шил\\n половину\\n ко\\n ##л\\n ##ча\\n ##на\\n и\\n ни\\n разу\\n не\\n про\\n ##махнулся\\n но\\n его\\n стрелы\\n одна\\n за\\n другои\\n от\\n ##ска\\n ##кивали\\n от\\n г\\n ##об\\n ##лин\\n ##ских\\n до\\n ##спех\\n ##ов\\n кое\\n что\\n кажется\\n за\\n ##стр\\n ##яло\\n во\\n в\\n ##ра\\n ##жье\\n ##и\\n ш\\n ##ку\\n ##ре\\n но\\n что\\n им\\n и\\n ##с\\n ##ча\\n ##дь\\n ##ю\\n ан\\n ##г\\n ##мара\\n эльфииские\\n стрелы\\n не\\n с\\n ##тра\\n ##шнее\\n ш\\n ##иль\\n ##ных\\n ты\\n ##чков\\n за\\n ##говор\\n ##енные\\n что\\n ли\\n с\\n с\\n ##у\\n ##ев\\n ##ер\\n ##ным\\n ужасом\\n бо\\n ##рм\\n ##о\\n ##чет\\n к\\n ##э\\n ##би\\n ##д\\n ##ж\\n с\\n ##вин\\n ##цов\\n ##ые\\n шар\\n ##ики\\n из\\n его\\n п\\n ##ра\\n ##щи\\n от\\n ##лета\\n ##ют\\n от\\n гром\\n ##ад\\n ##ин\\n г\\n ##об\\n ##лин\\n ##ов\\n не\\n причин\\n ##яя\\n им\\n ни\\n малеишего\\n вреда\\n ж\\n ##уть\\n какая\\n г\\n ##об\\n ##лины\\n п\\n ##ер\\n ##еро\\n ##ст\\n ##ки\\n они\\n же\\n не\\n бывают\\n такими\\n з\\n ##дор\\n ##овыми\\n просто\\n не\\n бывают\\n никогда\\n никогда\\n'], ['а\\n ##г\\n ##дал\\n ##он\\n мы\\n все\\n ж\\n ##дем\\n тебя\\n сеичас\\n капитан\\n сеичас\\n сеичас\\n в\\n с\\n ##ы\\n ##рую\\n по\\n ##го\\n ##ду\\n маги\\n ##ческии\\n посох\\n не\\n работа\\n ##ет\\n один\\n из\\n г\\n ##об\\n ##лин\\n ##ов\\n на\\n р\\n ##ав\\n ##ных\\n р\\n ##уб\\n ##ился\\n с\\n д\\n ##ун\\n ##ада\\n ##ном\\n тор\\n ##ном\\n оба\\n р\\n ##ос\\n ##лые\\n м\\n ##ед\\n ##лит\\n ##ельные\\n и\\n могу\\n ##чие\\n об\\n ##ра\\n ##ба\\n ##тывал\\n ##и\\n друг\\n друга\\n то\\n ##чь\\n в\\n то\\n ##чь\\n два\\n добрых\\n кузнец\\n ##а\\n л\\n ##у\\n ##п\\n ##ящих\\n по\\n на\\n ##ковал\\n ##ь\\n ##не\\n те\\n тень\\n ба\\n ##м\\n те\\n тень\\n ба\\n ##м\\n те\\n тень\\n ба\\n ##м\\n кто\\n кого\\n во\\n ##зь\\n ##мет\\n из\\n ##мо\\n ##ром\\n да\\n ##нно\\n а\\n ##каи\\n ##н\\n ч\\n ##ерно\\n ##волосыи\\n к\\n ##ра\\n ##са\\n ##в\\n ##чик\\n получил\\n удар\\n г\\n ##об\\n ##лин\\n ##ским\\n кулаком\\n в\\n грудь\\n и\\n был\\n бы\\n кулак\\n г\\n ##ол\\n ##ым\\n пол\\n ##бе\\n ##ды\\n а\\n то\\n ведь\\n на\\n нем\\n п\\n ##ер\\n ##чат\\n ##ка\\n с\\n м\\n ##ед\\n ##ными\\n наши\\n ##вка\\n ##ми\\n лежит\\n ар\\n ##нор\\n ##ец\\n в\\n к\\n ##уст\\n ##ах\\n сто\\n ##нет\\n подняться\\n про\\n ##бу\\n ##ет\\n и\\n толку\\n от\\n него\\n р\\n ##овным\\n счет\\n ##ом\\n никакого\\n тол\\n ##к\\n если\\n и\\n есть\\n то\\n исключительно\\n от\\n двух\\n боицов\\n сам\\n капитан\\n ха\\n ##д\\n ##дар\\n молча\\n от\\n ##бив\\n ##ается\\n от\\n второго\\n г\\n ##об\\n ##лина\\n раза\\n в\\n два\\n выше\\n его\\n ростом\\n и\\n раза\\n в\\n пол\\n ##тора\\n шире\\n щит\\n его\\n р\\n ##ас\\n ##ко\\n ##лот\\n л\\n ##евая\\n рука\\n в\\n ##ис\\n ##ит\\n п\\n ##лет\\n ##ью\\n из\\n п\\n ##ред\\n ##п\\n ##ле\\n ##чья\\n х\\n ##ле\\n ##ще\\n ##т\\n кровь\\n дела\\n капитана\\n совсем\\n не\\n ##хо\\n ##ро\\n ##ши\\n'], ['смерть\\n гляди\\n ##т\\n ему\\n в\\n самые\\n о\\n ##чи\\n и\\n у\\n ##х\\n ##мы\\n ##ляется\\n гном\\n та\\n ##бар\\n ##ин\\n отчаянно\\n с\\n ##к\\n ##верно\\n ##с\\n ##лов\\n ##я\\n р\\n ##ас\\n ##се\\n ##р\\n ##женным\\n п\\n ##ет\\n ##у\\n ##шком\\n нас\\n ##ка\\n ##ки\\n ##вает\\n на\\n третьего\\n г\\n ##об\\n ##лина\\n а\\n ##г\\n ##дал\\n ##он\\n сеичас\\n во\\n от\\n гром\\n ##кии\\n ш\\n ##ип\\n и\\n от\\n в\\n ##ерх\\n ##ушки\\n его\\n посох\\n ##а\\n от\\n ##рыва\\n ##ется\\n легко\\n ##е\\n об\\n ##ла\\n ##чко\\n пара\\n по\\n об\\n ##ли\\n ##чи\\n ##ю\\n своему\\n пар\\n этот\\n к\\n ##ров\\n ##ная\\n род\\n ##ня\\n ба\\n ##нному\\n третии\\n г\\n ##об\\n ##лин\\n л\\n ##овки\\n ##м\\n ударом\\n с\\n ##бив\\n ##ает\\n с\\n та\\n ##бар\\n ##ина\\n шлем\\n но\\n тот\\n ничуть\\n не\\n за\\n ##ме\\n ##д\\n ##ляет\\n б\\n ##е\\n ##шены\\n ##х\\n нас\\n ##ко\\n ##ков\\n наконец\\n б\\n ##е\\n ##ат\\n ##ор\\n при\\n ##з\\n ##вав\\n илуватара\\n и\\n все\\n его\\n небе\\n ##сное\\n воин\\n ##ство\\n на\\n помощь\\n б\\n ##ье\\n ##т\\n де\\n ##рев\\n ##ян\\n ##ным\\n мол\\n ##от\\n ##ком\\n по\\n не\\n ##по\\n ##с\\n ##лу\\n ##шно\\n ##му\\n ворот\\n ##ку\\n тот\\n раз\\n ##лета\\n ##ется\\n на\\n части\\n со\\n ##д\\n ##ро\\n ##гается\\n с\\n ##тан\\n ##ина\\n ба\\n ##лл\\n ##ист\\n ##ы\\n и\\n с\\n в\\n ##ерх\\n ##него\\n само\\n ##стр\\n ##ела\\n с\\n ##ходит\\n тяжелое\\n м\\n ##ета\\n ##тельно\\n ##е\\n копье\\n зато\\n на\\n ни\\n ##ж\\n ##нем\\n за\\n ##еда\\n ##ет\\n п\\n ##ус\\n ##ков\\n ##ои\\n к\\n ##рю\\n ##к\\n копье\\n лет\\n ##ит\\n кажется\\n прямо\\n в\\n лоб\\n г\\n ##об\\n ##лину\\n с\\n ##ра\\n ##жа\\n ##юще\\n ##му\\n ##ся\\n с\\n ха\\n ##д\\n ##дар\\n ##ом\\n но\\n потом\\n оно\\n по\\n ##падает\\n почему\\n то\\n совсем\\n не\\n в\\n лоб\\n а\\n в\\n огромное\\n г\\n ##об\\n ##лин\\n ##ское\\n ухо\\n']]\n"
     ]
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(ru_vocab, token_batch)\n",
    "# Join with spaces.\n",
    "joined_txt = tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)\n",
    "\n",
    "decoded_list = [decode_string(ex) for ex in tf.strings.unicode_decode(joined_txt, 'utf-8').numpy()]\n",
    "print(decoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  # Drop the reserved tokens, except for \"[UNK]\".\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "  # Join them into strings.\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(1,-1)\n",
    "    # enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ru_tokenizer'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7653"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.ru.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Examples in Rus:\n",
      "множество творений дивных и странных оставалось еще в мире но немало было и лихого жуткого орки тролли драконы и хищные твари в лесах бродили неведомые и мудрые создания гномы трудились в горах терпеливым искусством творя из металла и камня вещи которым не было равных но близилось владычество людей и все менялось вороток на баллисте никак не хотел отпустить тетиву беатор в отчаянии ковырял его ножом впрочем безо всякого успеха эльф опустошил половину колчана и ни разу не промахнулся но его стрелы одна за другой отскакивали от гоблинских доспехов кое что кажется застряло во вражьей шкуре но что им исчадью ангмара эльфийские стрелы не страшнее шильных тычков заговоренные что ли с суеверным ужасом бормочет кэбидж свинцовые шарики из его пращи отлетают от громадин гоблинов не причиняя им ни малейшего вреда жуть какая гоблины переростки они же не бывают такими здоровыми просто не бывают никогда никогда \n",
      " агдалон мы все ждем тебя сейчас капитан сейчас сейчас в сырую погоду магический посох не работает один из гоблинов на равных рубился с дунаданом торном оба рослые медлительные и могучие обрабатывали друг друга точь в точь два добрых кузнеца лупящих по наковальне те тень бам те тень бам те тень бам кто кого возьмет измором данно акайн черноволосый красавчик получил удар гоблинским кулаком в грудь и был бы кулак голым полбеды а то ведь на нем перчатка с медными нашивками лежит арнорец в кустах стонет подняться пробует и толку от него ровным счетом никакого толк если и есть то исключительно от двух бойцов сам капитан хаддар молча отбивается от второго гоблина раза в два выше его ростом и раза в полтора шире щит его расколот левая рука висит плетью из предплечья хлещет кровь дела капитана совсем нехороши \n",
      " смерть глядит ему в самые очи и ухмыляется гном табарин отчаянно сквернословя рассерженным петушком наскакивает на третьего гоблина агдалон сейчас во от громкий шип и от верхушки его посоха отрывается легкое облачко пара по обличию своему пар этот кровная родня банному третий гоблин ловким ударом сбивает с табарина шлем но тот ничуть не замедляет бешеных наскоков наконец беатор призвав илуватара и все его небесное воинство на помощь бьет деревянным молотком по непослушному воротку тот разлетается на части содрогается станина баллисты и с верхнего самострела сходит тяжелое метательное копье зато на нижнем заедает пусковой крюк копье летит кажется прямо в лоб гоблину сражающемуся с хаддаром но потом оно попадает почему то совсем не в лоб а в огромное гоблинское ухо \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ru_ex in keras_dataset.batch(3).take(1):\n",
    "  print('> Examples in Rus:')\n",
    "  for ru in ru_ex.numpy():\n",
    "    for i in ru:\n",
    "      print(i.decode('utf-8'))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is a padded-batch of token IDs:\n",
      "[1503, 26, 773, 2272, 332, 13, 600, 167, 17, 6850, 1192, 70, 11, 746, 45, 1780, 64, 17, 116, 6978, 114, 15, 656, 2006, 726, 3280, 4325, 17, 29, 5478, 150, 2467, 11, 4924, 10, 4543, 393, 40, 1165, 1753, 277, 17, 20, 3950, 4021, 4964, 561, 26, 4921, 504, 11, 2199, 3340, 3441, 2723, 6467, 58, 26, 773, 2362, 51, 5072, 17, 1083, 1346, 948, 40, 64, 24, 2492, 167, 45, 10, 139, 5264, 11, 7132, 77, 1515, 280, 17, 52, 86, 1394, 1062, 197, 41, 6754, 3819, 3985, 53, 957, 40, 347, 60, 1907, 348, 6197, 62, 10, 53, 1745, 554, 11, 3878, 702, 839, 2362, 124, 48, 4712, 620, 170, 75, 3443, 27, 6523, 44, 672, 22, 1556, 5463, 2175, 4535, 702, 124, 625, 149, 17, 85, 2427, 40, 569, 4310, 45, 48, 1214, 416, 57, 357, 60, 1639, 6079, 60, 12, 5579, 1356, 455, 87, 6523, 93, 482, 42, 498, 57, 7103, 2936, 130, 11, 270, 6948, 46, 32, 179, 627, 45, 42, 138, 17, 199, 625, 1961, 106, 5133, 330, 4986, 3035, 1214, 40, 25, 3492, 3565, 32, 2638, 167, 54, 6327, 57, 3643, 1269, 42, 116, 25, 25, 62, 686, 724, 161, 4929, 5335, 4146, 75, 1493, 18, 615, 2527, 251, 1278, 25, 4531, 2000, 277, 4656, 1559, 51, 48, 23, 270, 2128, 60, 7025, 372, 60, 6058, 2733, 339, 12, 5579, 1356, 93, 40, 7498, 1473, 138, 85, 6276, 6112, 15, 1988, 617, 12, 5579, 3322, 23, 724, 3766, 1148, 115, 55, 65, 40, 6220, 2043, 16, 2724, 5462, 169, 40, 6220, 219, 219]\n",
      "[9, 330, 911, 541, 73, 52, 15, 2325, 103, 142, 1719, 142, 142, 11, 25, 77, 3971, 56, 114, 588, 6245, 2548, 2446, 40, 2658, 173, 140, 51, 12, 5579, 1356, 93, 41, 24, 2492, 167, 24, 6752, 386, 25, 13, 2256, 6646, 371, 3125, 371, 935, 24, 3273, 1677, 20, 3887, 3439, 6077, 17, 321, 1520, 214, 270, 858, 7197, 46, 316, 527, 49, 766, 11, 49, 766, 319, 6984, 4326, 44, 19, 62, 557, 2091, 56, 41, 6097, 146, 267, 348, 775, 6754, 58, 348, 775, 6754, 58, 348, 775, 6754, 58, 95, 344, 130, 5266, 3440, 51, 1494, 1242, 91, 1115, 9, 6372, 104, 31, 3598, 5459, 18, 270, 419, 177, 1792, 1656, 929, 12, 5579, 1356, 1106, 4662, 11, 1538, 17, 74, 66, 4772, 12, 1276, 301, 1061, 1541, 857, 9, 49, 185, 41, 366, 23, 724, 2665, 132, 25, 20, 3887, 326, 769, 4945, 83, 2182, 5304, 4791, 556, 11, 18, 6927, 290, 2361, 575, 2745, 569, 1234, 173, 17, 3011, 60, 98, 24, 5749, 4008, 67, 1104, 7469, 159, 71, 17, 143, 49, 5754, 60, 610, 6575, 193, 1719, 2754, 251, 4982, 684, 60, 5283, 1325, 60, 3787, 12, 5579, 3377, 2980, 11, 319, 958, 48, 3642, 17, 2980, 11, 1061, 4802, 3367, 3379, 48, 24, 2913, 492, 2755, 19, 4307, 802, 11, 2155, 237, 23, 1332, 827, 51, 23, 1919, 557, 598, 1965, 29, 598, 681, 101, 623, 693, 7212, 243, 40, 6978, 1472, 1041]\n",
      "[797, 7567, 101, 84, 11, 1424, 22, 674, 17, 27, 80, 1440, 4282, 535, 597, 5474, 339, 2332, 25, 159, 5825, 199, 2302, 92, 24, 2913, 1195, 155, 2786, 23, 173, 62, 4575, 127, 132, 115, 959, 41, 7057, 12, 5579, 3377, 9, 330, 911, 541, 142, 130, 60, 6058, 722, 32, 6834, 17, 60, 11, 7023, 2441, 48, 2446, 44, 60, 6052, 547, 777, 53, 214, 135, 2810, 1962, 56, 214, 139, 674, 106, 1048, 874, 145, 18, 1051, 191, 4402, 655, 6754, 5026, 1790, 12, 5579, 1356, 19, 7289, 58, 3809, 25, 5283, 520, 25, 597, 5474, 634, 3390, 45, 181, 3371, 40, 57, 1498, 251, 2094, 10, 53, 4671, 80, 127, 492, 418, 228, 10, 53, 1745, 554, 229, 515, 3050, 6846, 17, 52, 48, 1906, 5899, 1288, 420, 41, 907, 10, 862, 101, 2854, 5532, 3603, 161, 3258, 1443, 281, 56, 40, 3733, 199, 918, 1990, 294, 1062, 179, 181, 121, 7025, 547, 41, 1741, 129, 251, 1472, 6900, 25, 2591, 634, 6754, 3819, 3985, 77, 17, 25, 11, 7023, 1492, 2300, 7103, 808, 25, 2559, 7279, 20, 6501, 1769, 53, 2069, 1663, 41, 85, 1278, 953, 57, 3120, 173, 23, 4284, 418, 148, 18, 3587, 159, 2069, 383, 237, 498, 345, 11, 2405, 12, 5579, 5783, 25, 270, 1334, 2965, 294, 63, 25, 2754, 251, 4982, 67, 45, 119, 287, 56, 6128, 184, 49, 243, 40, 11, 2405, 9, 11, 4073, 12, 5579, 1356, 1371, 3104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 12:08:53.124972: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: StatefulPartitionedCall/RaggedFromUniformRowLength/RowPartitionFromUniformRowLength/assert_greater_equal/Assert/AssertGuard/branch_executed/_113\n"
     ]
    }
   ],
   "source": [
    "encoded = reloaded_tokenizers.ru.tokenize(ru_ex)\n",
    "\n",
    "print('> This is a padded-batch of token IDs:')\n",
    "i = 0\n",
    "for row in encoded.to_list():\n",
    "  i += 1\n",
    "  print(row)\n",
    "  if i == 3:\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
