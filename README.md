# Text Generator

Цель: создать русскоязычный текстовый генератор, разобраться в архитектуре трансформеров. (Изначальной целью было создать генерацию описаний фантастических созданий, дополнить другими моделями машинного обучения)

Библиотеки: 
- tensorflow (+keras, tensorflow_text), pandas
- BeautifulSoup, requests, asyncio, aiohttp
- langdetect
- Вспомогательные: pickle, cProfile, numpy, re

# Первая модель 
- Изначально был собран датасет с трёх сайтов: 
	- bestiary.su
	- dnd.su
	- zoopicture.ru (добавлен позже)
<p>Размер: 5380 строк

- Под каждый сайт заведен свой парсер и своя очистка текстов. 
	- т.к. выбранные сайты не предоставляли json при парсинге, выбор содержательных фрагментов был индивидуальным, в зависимости от разметки конкретного сайта
	- Были выбраны эти сайты, т.к. была цель приблизить выбранные тексты по смыслу и стилю к финальной генерации.

- Датасет был предобработан и очищен 
- Обучена модель 
	- Встроенная векторизация keras
	- positional embedding
	- decoder
	- Оптимизатор rmsprop
	- Генерация с разными температурами

Полученные результаты (как мне показалось на тот момент по растущему val loss) указывали на переобучение модели, данная архитектура и датасет не дали удовлетворительного результата. 
## Пример генерации
```
# 5380 строк обучающая выборка
#prompt = "розовый коловрат"
Epoch 20/50 loss: 0.7111 - val_loss: 1.1331 #Дальше val_loss только растет
==Generating with temperature 0.2 == 
розовый коловрат или шеями причем длинными глубокую великанами полосу один одиночных из смотрящих коренного на классический глубине европы около почти двух невозможно тысяч кто икринок изменил в 
==Generating with temperature 0.5 == 
розовый коловрат все бесчисленными [UNK] получать совсем существо отбрасывать в врагов мире и по единственный [UNK] экземпляр для был [UNK] раз также [UNK] весьма отсюда среднее редкое
==Generating with temperature 0.7 == 
розовый коловрат здания эту сила машину странного для паука себя на на ее гибридном жителей теле десяти своего ног рода генри яблоки [UNK] [UNK] голову [UNK] клыки
==Generating with temperature 1 == 
розовый коловрат странствующий числятся рыцарь мелкие вместо итальянские того [UNK] ко [UNK] всем на своим немецком противникам и что несла кто небольшие есть жучки жару к они 
==Generating with temperature 1.5 == 
розовый коловрат варвары представитель у рода металла злобная и титул [UNK] мыслился на черная хвосте кувшине несколько [UNK] десятков или дополнительных вполне самых рассматривают опасных большинство смертных
```
Обучение этой модели на собранном позже датасете на 100+ тыс. строк также не показало хотя бы приближенной к реальности генерации. 
```
Epoch 4/10
4500/4500 [==============================] - ETA: 0s - loss: 0.3509==Generating with temperature 0.2 ==
Розовый коловрат на который [UNK] держал [UNK] его [UNK] оторванный [UNK] но [UNK] все и это не весь двигался слепок где в над плащ любовником [UNK] даже
==Generating with temperature 0.5 ==
Розовый коловрат и веско [UNK] золоченых [UNK] и сеновала замешательство пес спешивались [UNK] и милость живые [UNK] убил там но совершенно для тяжело цири дыша видя [UNK]
==Generating with temperature 0.7 ==
Розовый коловрат [UNK] хоть ангмар как фыркала случайности [UNK] можно еще лучше [UNK] рейнмар водой фон своим [UNK] [UNK] [UNK] но виттих о я днях решился самого
==Generating with temperature 1 ==
Розовый коловрат уже кум много плотва [UNK] так включил никуда [UNK] не уважения пострадал вверх как когда ощущался помалкивать половину при [UNK] одном для только [UNK] о
==Generating with temperature 1.5 ==
Розовый коловрат ароматы это носовой не бледных [UNK] потому  что кто когда ты сказала она хотела стать она пару мелочей с тобой менестрелю мелочами я должна
```
____
# Вторая модель
- Далее был собран датасет на базе сборников Толкиена и Сапковского, fb2 -> pandas
<p>Размер: 106793 строк (произведения были разделены на части ⩽2000 символов)

- Тексты очищены 
- В модель добавлено больше слоев, оптимизатор - adam
- Слой векторизации - subword vectorizer на базе bert 

Время обучения модели сильно возросло (~30 минут на эпоху), десять эпох обучения не дали удовлетворительных результатов. Выбранный subword vecorizer неудачно влияет на работу модели.

## Пример генерации

```
Generating 50 tokens
Using temperature of 0.92
Input:         : Злобный рыцарь
Generation     :
 [UNK] они ##жения ##пешно вдруг б парень был вечером б ##ит ##сек ##ниц ##ую было использовать на что она мне та такого угодно она б ##итись ##щные отдохнуть острова она ##нимали тем ##лены она ##еешь ##вои ##вет и видел ##очкои рядом каждыи него уничтожить это р раздался ##цы ##го з стояли
```
![Pasted image 20240320154553](https://github.com/dariamozglova/TextGenerator/assets/107386336/b0ad1274-e525-43f8-a5d3-b6f3fc236480)
![Pasted image 20240320154609](https://github.com/dariamozglova/TextGenerator/assets/107386336/7c19d8b8-a182-447d-b773-e6e36faa1cee)

<p>Т.к. subword vectorizer сильно увеличил время необходимого обучения (либо просто не давал нужного результата), решила от него временно отказаться и, до рассмотрения альтернатив, попробовать обучить бОльшую модель на большом датасете. 

```
Generating 50 tokens (обучающая выборка 20тыс. строк, 120 эпох)
Using temperature of 0.92
Input:         : Злобный рыцарь
Generation     :
 [UNK] с седла широко улыбнулся он по прежнему был во вторых слова  меня под уши под  и   кого я имею в виду не мог  уверен мне уши у каждого государства три  поэтому я не вернусь нет сказал аурион он и я должен был подать сигнал
```

<blockquote>
<b>#TODO:</b> 
<li>В дальнейшем хочу попробовать изменить слой векторизации, возможно попробовать морфемный анализ для разделения слов на токены или рассмотреть другие subword токенизаторы. </li>
<li>Возможно, попробовать генерацию на базе LSTM, а не трансформера. </li>
<li>Если и этот вариант не подойдет, тогда вижу смысл в дообучении русскоязычных GPT из открытого доступа. </li></blockquote>

----
- Самый крутой челлендж создал фанатский сайт bestiary.su. 
	- Неструктурированные страницы, отсутствие обособленных тегов и достаточно частые повторы (как внутри страниц, так и среди разных страниц) вынудили с большим вниманием подойти к постобработке собранных данных (из 4349 страниц сайта содержательными оказалось меньше 2 тысяч).
	- Особенно заняла фильтрация славянских текстов, ведь любая предобученная модель вроде langdetect воспринимала их как русские ![😁](https://vk.com/emoji/e/f09f9881.png). 
	- Медленный сервер вынудил пользоваться asyncio (ожидание ответа занимало до 10 секунд).
	- В какой-то момент я даже пыталась убрать упоминания авторов POS-tagging'ом, т.к. единого стиля оформления на сайте не предусмотрено
- У dnd.su стояла простенькая защита от парсинга, которая скидывала соединение спустя несколько запросов. Получилось обойти простым решением с листом ссылок, которые отбивали ошибки через try/except.
- Вторая модель точно что-то знает о переживаниях эльфов в "Хоббите")) 
![image_2024-03-21_12-17-02](https://github.com/dariamozglova/TextGenerator/assets/107386336/dcc1559f-fddd-4f85-b953-5a50e9133c89)
